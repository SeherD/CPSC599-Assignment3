A report (maximum two pages) highlighting different approaches to improving your model and avoiding overfitting.
- count the number of classes and training samples per class
- 10 classes, 28 - 49 samples per class
- not very many training samples, we can rotate, flip, blur, or manipulate the color balance of the image in a way that the label remains valid and train with many epochs to give the effect of an increased number of training samples
- to make the images the same size, we can stretch the images with a resize crop to 214 x 214

- since we do not have a very large dataset and data augmentation will only go so far, we will try to transfer learning with the StanfordCars cars dataset.
- we have a small dataset that is quite similar to the larger dataset, so we can train the model on the large dataset, freeze all but the last few layers, and use the features learned on the larger dataset to train the classifier for this dataset.

- we will try to use the https://pytorch.org/vision/stable/generated/torchvision.datasets.StanfordCars.html#torchvision.datasets.StanfordCars cars dataset to train the earlier layers that will identify features in images like shapes and lines and edges and car features.

- rather than use dropout, we will try to use cutmix and mixup as additional data augmentation techniques to prevent memorization of the exact samples in the dataset and overfitting: https://pytorch.org/vision/stable/auto_examples/transforms/plot_cutmix_mixup.html#sphx-glr-auto-examples-transforms-plot-cutmix-mixup-py
we can also use cutmix and mixup 